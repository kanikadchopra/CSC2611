{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350a0373",
   "metadata": {},
   "source": [
    "# Meaning Construction from Text\n",
    "**CSC2611** \n",
    "\n",
    "Kanika Chopra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5532ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7d719",
   "metadata": {},
   "source": [
    "#### Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac0fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19531210",
   "metadata": {},
   "source": [
    "##### Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6638ed46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29fa17fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41dafecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498d283a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df917c",
   "metadata": {},
   "source": [
    "We have 1,162, 192 words in our corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5507a",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "\n",
    "First, we want to extract the 5000 most common English words (denoted W) based on unigram frequencies in the Brown corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d15102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unigrams (words)\n",
    "words = brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d14595",
   "metadata": {},
   "source": [
    "To clean the data before we get the unigrams, we want to remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9871e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "words = [word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e68b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Frequency Distribution\n",
    "fdist = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9843ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 5000 most common words\n",
    "W = [pair[0].lower() for pair in fdist.most_common(5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca63201",
   "metadata": {},
   "source": [
    "Next, we want to report the 5 most and least common words we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9011c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'and']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words \n",
    "W[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db0d015c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expanded', 'emphasize', 'manhattan', 'temporarily', 'puts']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least common words \n",
    "W[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a82773",
   "metadata": {},
   "source": [
    "Then, we update W by adding n words where n is the set of words in Table 1 of RG65 that were not included in the top 5000 words from the Brown corpus. Denote the total number of words in W as |W|.\n",
    "\n",
    "Hence, we need to get a list of the words from Table 1 and then compare to see which words are already in W. If they are, we skip the word. If they aren't, we add it to our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "adb8581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg65 = pd.read_csv('rg65.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "46a0551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = np.array(rg65[['word1', 'word2']]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19cd20",
   "metadata": {},
   "source": [
    "First, we want to remove duplicates from our table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "40057dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = np.unique(table1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdddc33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6c943333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55826f1d",
   "metadata": {},
   "source": [
    "Currently, we have that table 1 has 48 words and W has 5000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae4e02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = [word for word in new_words if word not in W]\n",
    "W.extend(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff31b098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5031"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a836f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc1d6b",
   "metadata": {},
   "source": [
    "We have that |W| = 5031. Hence, there were 31 words that were not in the original corpus that were in Table 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aeac6",
   "metadata": {},
   "source": [
    "#### Section 3\n",
    "We want to construct a word-context model (denoted M1) by collecting bigram counts for the words in W. This should become a |W| x |W| matrix where each word is a row in W and each column is a context in W that precedes row words in sentences. \n",
    "\n",
    "For example, if the phrase _taxi driver_ appears 5 times in the entire corpus, then the row _taxi_, column _driver_ would have a value of 5 in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e3bd19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bigrams\n",
    "bwbg = bigrams(words)\n",
    "\n",
    "# Get the Frequency Distribution\n",
    "fdist_bwbg = nltk.FreqDist(bwbg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "71a1d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('of', 'the'): 9625, (',', 'and'): 6288, ('.', 'The'): 6081, ('in', 'the'): 5546, (',', 'the'): 3754, ('.', '``'): 3515, ('to', 'the'): 3426, (\"''\", '.'): 3332, (';', ';'): 2784, ('.', 'He'): 2660, ...})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_bwbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cd0d28a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_bwbg[('the', 'driver')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dfe44",
   "metadata": {},
   "source": [
    "Now, we want to create our word-context model M1 for the bigram counts for words in W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8b1942f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = np.zeros((len(W), len(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "19bb533c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through words in W and count frequencies\n",
    "for i, word1 in enumerate(W):\n",
    "    for j, word2 in enumerate(W):\n",
    "        M1[i,j] = fdist_bwbg[(word1, word2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97ce3f",
   "metadata": {},
   "source": [
    "#### Section 4: Positive Pointwise Mutual Information\n",
    "Using M1, we want to compute positive pointwise mutual information and denote this as M1+."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e712eb",
   "metadata": {},
   "source": [
    "We want to only transform the values where our count is non-zero. When the count is zero, we would end up with -$\\infty$ so we ignore these since we would set negative values to 0 in PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b34b49be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05400743374050114"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.freq('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "82ce29fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1688414.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6c46207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1_plus_tmp = np.zeros((len(W), len(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a6b9c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(fdist_bwbg)\n",
    "for i in range(len(W)):\n",
    "    for j in range(len(W)):\n",
    "        Pwc = M1[i,j]/n\n",
    "        Pc = fdist.freq(W[i])\n",
    "        Pw = fdist.freq(W[j])\n",
    "        if (Pc != 0) & (Pw != 0) & (M1[i,j] != 0):\n",
    "            M1_plus_tmp[i,j] = max(math.log(Pwc/(Pc*Pw),2),0)\n",
    "        else:\n",
    "            M1_plus_tmp[i,j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9743e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1_plus = lil_matrix(M1_plus_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8c470",
   "metadata": {},
   "source": [
    "#### Section 5: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "411bcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "M2_10 = PCA(n_components=10).fit_transform(M1_plus_tmp)\n",
    "M2_100 = PCA(n_components=100).fit_transform(M1_plus_tmp)\n",
    "M2_300 = PCA(n_components=300).fit_transform(M1_plus_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be4ea7",
   "metadata": {},
   "source": [
    "#### Section 6: \n",
    "Next, we want to find all pairs of words in Table 1 of RG65 that are also available in W . Denote these pairs as P . Record the human-judged similarities of these word pairs from the table and denote similarity values as S.\n",
    "\n",
    "Since, we have added all of the words in Table 1 that are not in W, we can simply use all of the pair combinations as P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0ff48d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = []\n",
    "P = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ae00b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(rg65.shape[0]):\n",
    "    word1 = rg65.word1[i]\n",
    "    word2 = rg65.word2[i]\n",
    "    \n",
    "    if (word1 in W) & (word2 in W):\n",
    "        pair = (word1, word2)\n",
    "        P.append(pair)\n",
    "        s = rg65.similarity[i]\n",
    "        S.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8ad9497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 65)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(S), len(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c6c97",
   "metadata": {},
   "source": [
    "#### Step 7: Cosine Similarity\n",
    "We want the cosine similarity between each pair of words in P based on the constructed word vectors for. We store these in SM1, SM1_plus, SM210, SM2100, SM2300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7e304",
   "metadata": {},
   "source": [
    "**Something is wrong with cosine similarity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1b964b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(M, W, P):\n",
    "    sim = []\n",
    "    for (word1, word2) in P:\n",
    "        arr1 = M[np.where(W == word1)[0]]\n",
    "        arr2 = M[np.where(W == word2)[0]]\n",
    "        val = cosine_similarity(arr1, arr2)[0][0]\n",
    "        sim.append(val)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8e07c097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SM1 = cos_sim(M1, W, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "154e2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM1_plus = cos_sim(M1_plus, W, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "45621ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM2_10 = cos_sim(M2_10, W, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eda12684",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM2_100 = cos_sim(M2_100, W, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d3af3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM2_300 = cos_sim(M2_300, W, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8357664",
   "metadata": {},
   "source": [
    "#### Step 8: Pearson Correlation\n",
    "Lastly, we want to report the Pearson correlation between S and each of our model-predicted similarities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48bcdf",
   "metadata": {},
   "source": [
    "##### S vs. $SM_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c912609d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20088035191717596, 0.10859784707272817)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(S, SM1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225ec7b",
   "metadata": {},
   "source": [
    "##### S vs. $SM_{1+}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4bbf562c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28743460050663483, 0.02025265881547502)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(S, SM1_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fee0b4",
   "metadata": {},
   "source": [
    "##### S vs. $SM_{2_{10}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "efded753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1364997946513758, 0.2782638242188669)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(S, SM2_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14178e",
   "metadata": {},
   "source": [
    "##### S vs. $SM_{2_{100}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fd0ee5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.30373949422240853, 0.013904293173821856)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(S, SM2_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf45f0",
   "metadata": {},
   "source": [
    "##### S vs. $SM_{2_{300}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fd7bfba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29904592325561313, 0.015526426496618749)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(S, SM2_300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
